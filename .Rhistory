if (example == "simple") {
if (d == 2) {
B <-
matrix(c(2,-1, 0.5, 0, 0, 0), ncol = 3) # 2 x 3 matrix with chosen coefficients
mu <- c(0, 0) # mean for the random noise
Sigma <-
matrix(c(1, 0.5, 0.5, 1), nrow = 2) # covariance matrix for the random noise
epsilon <-
rmvnorm(n, mu, Sigma) # generate random noise following a bivariate normal distribution
# Create the 2-dimensional response variable Y
Y <-
as.matrix(X %*% t(B) + epsilon) # compute Y as the linear combination of X and B plus random noise
} else if (d == 1) {
# one-dimensional
B <- matrix(c(6, 3, 0), ncol = 3)
epsilon <- rnorm(n)
## Create the 2-dimensional response variable Y
Y <-
as.matrix(X %*% t(B) + epsilon) # compute Y as the linear combination of X and B plus random noise
}
} else if (example == "fancy") {
# more fancy example
sigX = X[, 2]
Y <- matrix(rnorm(n, mean = 0, sd = sqrt(sigX)), nrow = n)
}
if (sample.splitting == T) {
# Sample Splitting
Xtest <- X[(round(n - ntest) + 1):n, , drop = F]
Ytest <- Y[(round(n - ntest) + 1):n, , drop = F]
#
X <- X[1:round(n - ntest), , drop = F]
Y <- Y[1:round(n - ntest), , drop = F]
} else{
# No sample splitting
Xtest <- X
Ytest <- Y
}
B <- 20
num.trees <- 1000
alpha <- 0.05
bandwidth_Y <- drf:::medianHeuristic(Ytest)
k_Y <- rbfdot(sigma = bandwidth_Y)
K <- kernelMatrix(k_Y, Y, y = Y)
DRF <-
drfCI(
X = X,
Y = Y,
B = B,
num.trees = num.trees
)
# Prediction with all X
DRFpred = predictdrf(DRF, x = Xtest)
wall <- DRFpred$weights
##
I0 <- sapply(1:ncol(Xtest), function(j) {
# iterate over class 1
## With CI
DRFj <-
drfCI(
X = X[, -j],
Y = Y,
B = B,
num.trees = num.trees
)
DRFpredj = predictdrf(DRFj, x = Xtest[, -j])
wj <- DRFpredj$weights
val <- mean(diag((wj - wall) %*% K %*% t(wj - wall)))
if (B > 1) {
# Get null distribution if B > 1
nulldist <- sapply(1:B, function(b) {
# iterate over class 1
wbj <- DRFpredj$weightsb[[b]]
wb <- DRFpred$weightsb[[b]]
mean(diag((wb - wall - (wbj - wj)) %*% K %*% t(wb - wall - (wbj - wj))))
})
##
right_quantile <- quantile(nulldist, 1 - alpha)
max(val - unname(right_quantile), 0)
} else{
val
}
})
### Calculate average weight
wbar <- colMeans(wall)
#( I<-I0/as.numeric( mean(diag(  as.matrix(wall) %*% K %*% t( as.matrix(wall)) )) - colMeans(wall)%*%K%*%colMeans(wall) ) )
(I <-
I0 / as.numeric(mean(diag(
(wbar - wall) %*% K %*% t(wbar - wall)
))))
frac.ntest<-0.1
n<-1000
frac.ntest<-0.1
## Step 1: Get the dataset
tmp<-genData(dataset = "synthetic1", n = n, p = 10, meanShift = 1, sdShift = 1)
source("~/GitHub/DRFvarimporance/applications/univariate_comparison/genData.R")
## Step 1: Get the dataset
tmp<-genData(dataset = "synthetic1", n = n, p = 10, meanShift = 1, sdShift = 1)
tmp
y<-tmp$y
X<-tmp$X
y<-tmp$y
### if the dataset is synthetic, we can check the correct variable ordering
VIs<-drfwithVI(X, Y, B, num.trees=num.trees)
source("~/GitHub/DRFvarimporance/drfnew_v2.R")
source("~/GitHub/DRFvarimporance/drfnew_v2.R")
source("~/GitHub/DRFvarimporance/applications/univariate_comparison/genData.R")
### if the dataset is synthetic, we can check the correct variable ordering
VIs<-drfwithVI(X, Y, B=1, num.trees=100)
Y<-tmp$y
### if the dataset is synthetic, we can check the correct variable ordering
VIs<-drfwithVI(X, Y, B=1, num.trees=100)
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
source("drfnew_v2.R")
set.seed(10)
### Continue with more fancy examples!!!
n<-1000
frac.ntest<-0.1
## Step 1: Get the dataset
tmp<-genData(dataset = "synthetic1", n = n, p = 10, meanShift = 1, sdShift = 1)
X<-tmp$X
Y<-tmp$y
### if the dataset is synthetic, we can check the correct variable ordering
VIs<-drfwithVI(X, Y, B=1, num.trees=100)
### if it is a real dataset, I need to make a prediction function that successively filters
# Sample Splitting
Xtest <- X[(round(n - ntest) + 1):n, , drop = F]
Ytest <- Y[(round(n - ntest) + 1):n, , drop = F]
#
X <- X[1:round(n - ntest), , drop = F]
Y <- Y[1:round(n - ntest), , drop = F]
B <- 1
num.trees <- 1000
alpha <- 0.05
drfwithVI(X, Y, B, num.trees=num.trees)
source("drfnew_v2.R")
set.seed(10)
n<-1000
frac.ntest<-0.1
## Step 1: Get the dataset
tmp<-genData(dataset = "synthetic1", n = n, p = 10, meanShift = 1, sdShift = 1)
X<-tmp$X
Y<-tmp$y
### if the dataset is synthetic, we can check the correct variable ordering
VIs<-drfwithVI(X, Y, B=1, num.trees=100)
source("~/GitHub/DRFvarimporance/drfnew_v2.R")
source("~/GitHub/DRFvarimporance/drfnew_v2.R")
n<-200
frac.ntest<-0.1
## Step 1: Get the dataset
tmp<-genData(dataset = "synthetic1", n = n, p = 10, meanShift = 1, sdShift = 1)
X<-tmp$X
Y<-tmp$y
### if the dataset is synthetic, we can check the correct variable ordering
res<-drfwithVI(X, Y, B=1, num.trees=100)
source("~/GitHub/DRFvarimporance/drfnew_v2.R")
### if the dataset is synthetic, we can check the correct variable ordering
res<-drfwithVI(X, Y, B=1, num.trees=100)
res
res$VI
paste0("X", 1:ncol(X))
source("~/GitHub/DRFvarimporance/drfnew_v2.R")
### if the dataset is synthetic, we can check the correct variable ordering
res<-drfwithVI(X, Y, B=1, num.trees=100)
res$VI
colnames(X)
min(res$VI)
which.min(res$VI)
Xnew<-X[,-which.min(res$VI)]
Xnew
head(Xnew)
head(X)
which.min(res$VI)
names(which.min(res$VI))
source("~/GitHub/DRFvarimporance/drfnew_v2.R")
resreal<-featureeliminnation(X,Y)
sort(ressynth$VI)
resreal
resreal$which
### if it is a real dataset, I need to make a prediction function that successively filters
# Sample Splitting
Xtest <- X[(round(n - ntest) + 1):n, , drop = F]
Ytest <- Y[(round(n - ntest) + 1):n, , drop = F]
#
X <- X[1:round(n - ntest), , drop = F]
Y <- Y[1:round(n - ntest), , drop = F]
resreal<-featureeliminnation(X,Y)
ntest<-round(n*0.1)
### if it is a real dataset, I need to make a prediction function that successively filters
# Sample Splitting
Xtest <- X[(round(n - ntest) + 1):n, , drop = F]
Ytest <- Y[(round(n - ntest) + 1):n, , drop = F]
#
X <- X[1:round(n - ntest), , drop = F]
Y
Y<-as.matrix(tmp$y)
Y
#
X <- X[1:round(n - ntest), , drop = F]
Y <- Y[1:round(n - ntest), , drop = F]
resreal<-featureeliminnation(X,Y)
X
Y
ressynth
# Step 1: Predict
DRF<-drf(X,Y, num.trees=num.trees)
DRF
weights<-predict(DRF,Xtest)$weights
weights
w
w<-weigthts[1,]
w<-weigths[1,]
w<-weights[1,]
Ytest
Ytest
### if it is a real dataset, I need to make a prediction function that successively filters
# Sample Splitting
Xtest <- X[(round(n - ntest) + 1):n, , drop = F]
Ytest <- Y[(round(n - ntest) + 1):n, , drop = F]
n<-200
ntest<-round(n*0.1)
## Step 1: Get the dataset
tmp<-genData(dataset = "synthetic1", n = n, p = 10, meanShift = 1, sdShift = 1)
X<-tmp$X
Y<-as.matrix(tmp$y)
### if the dataset is synthetic, we can check the correct variable ordering
ressynth<-drfwithVI(X, Y, B=1, num.trees=100)
sort(ressynth$VI)
### if it is a real dataset, I need to make a prediction function that successively filters
# Sample Splitting
Xtest <- X[(round(n - ntest) + 1):n, , drop = F]
Ytest <- Y[(round(n - ntest) + 1):n, , drop = F]
#
X <- X[1:round(n - ntest), , drop = F]
Y <- Y[1:round(n - ntest), , drop = F]
y<-Ytest[1,]
y
# Simulate 500 observations from Y|X=x
Yx<-sample(Y,size=500, replace=T,prob=w)
Yx
mmd(y,Yx,sigma=1)
source("~/GitHub/DRFvarimporance/drfnew_v2.R")
mmd(y,Yx,sigma=1)
d<-function(w, Y,y){
# Simulate 500 observations from Y|X=x
Yx<-sample(Y,size=500, replace=T,prob=w)
return(mmd(y,Yx,sigma=1))
}
source("~/GitHub/DRFvarimporance/drfnew_v2.R")
eval<-distpredicteval(X,Y,Xtest, Ytest, DRF,d)
## Define a distance function D
d<-function(w, Y,y){
# Simulate 500 observations from Y|X=x
Yx<-sample(Y,size=500, replace=T,prob=w)
return(mmd(y,Yx,sigma=1))
}
eval<-distpredicteval(X,Y,Xtest, Ytest, DRF,d)
i<-1
weights[i,]
Y
d(weights[i,], Y, y[i,])
y
Ytest
debugSource("~/GitHub/DRFvarimporance/drfnew_v2.R")
eval<-distpredicteval(X,Y,Xtest, Ytest, DRF,d)
debugSource("~/GitHub/DRFvarimporance/drfnew_v2.R")
source("~/GitHub/DRFvarimporance/drfnew_v2.R")
eval<-distpredicteval(X,Y,Xtest, Ytest, DRF,d)
eval
libary(ks)
library(ks)
install.packages("ks")
library(ks)
# Simulate 500 observations from Y|X=x
Yx<-sample(Y,size=500, replace=T,prob=w)
density_est_multi <- kde(Yx)
density_est_multi
density_est_multi(y)
density_est <- kde(Yx)
kde.evaluate(density_est, y)
?evaluate
?kde
predict(density_est, y)
density_est <- kde(Yx, eval.points = y)
density_est
densityvaly <- kde(Yx, eval.points = y)$estimate
densityvaly
dNPLD <- function(w,Y,y){
# Simulate 500 observations from Y|X=x
Yx<-sample(Y,size=500, replace=T,prob=w)
densityvaly <- kde(Yx, eval.points = y)$estimate
return( - log(densityvaly))
}
eval<-distpredicteval(X,Y,Xtest, Ytest, DRF,dNPLD)
eval
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
library(ks)
source("drfnew_v2.R")
set.seed(10)
### Continue with more fancy examples!!!
n<-200
ntest<-round(n*0.1)
## Step 1: Get the dataset
tmp<-genData(dataset = "synthetic1", n = n, p = 10, meanShift = 1, sdShift = 1)
X<-tmp$X
Y<-as.matrix(tmp$y)
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
library(ks)
source("drfnew_v2.R")
set.seed(10)
n<-200
ntest<-round(n*0.1)
## Step 1: Get the dataset
tmp<-genData(dataset = "synthetic1", n = n, p = 10, meanShift = 1, sdShift = 1)
data("Ozone", package = "mlbench")
Ozone
Ozone <- subset(Ozone, complete.cases(Ozone))
Ozone
data("Ozone", package = "mlbench")
Ozone <- subset(Ozone, complete.cases(Ozone))
Ozone <- as.data.frame(lapply(Ozone, function(x) {
x <- x[, drop = TRUE]
if (is.factor(x)) return(as.ordered(x))
x
}))
response <- "V4"
Ozone[[response]] <- as.numeric(Ozone[[response]])
Ozone
response
drf
X
X
X<-tmp$X
Y<-as.matrix(tmp$y)
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
library(ks)
source("drfnew_v2.R")
source("applications")
set.seed(10)
### Continue with more fancy examples!!!
n<-200
ntest<-round(n*0.1)
## Step 1: Get the dataset
tmp<-genData(dataset = "synthetic1", n = n, p = 10, meanShift = 1, sdShift = 1)
X<-tmp$X
Y<-as.matrix(tmp$y)
source("~/GitHub/DRFvarimporance/drfnew_v2.R")
source("~/GitHub/DRFvarimporance/applications/univariate_comparison/genData.R")
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
library(ks)
source("drfnew_v2.R")
source("applications")
set.seed(10)
### Continue with more fancy examples!!!
n<-200
ntest<-round(n*0.1)
## Step 1: Get the dataset
tmp<-genData(dataset = "synthetic1", n = n, p = 10, meanShift = 1, sdShift = 1)
X<-tmp$X
Y<-as.matrix(tmp$y)
eval<-distpredicteval(X,Y,Xtest, Ytest, DRF,dNPLD)
X
dim(X)
X[1,10]
X[1,10]<-NA
X
head(X)
drf(X=X,Y=Y)
drf
X
head(X)
debug(drf)
drf(X=X,Y=Y)
sample.weights
clusters
all.tunable.params <- c("sample.fraction", "mtry", "min.node.size",
"honesty.fraction", "honesty.prune.leaves", "alpha",
"imbalance.penalty")
Y.transformed <- scale(Y)
data <- create_data_matrices(X.mat, outcome = Y.transformed,
sample.weights = sample.weights)
data
bandwidth <- medianHeuristic(Y.transformed)
args <- list(num.trees = num.trees, clusters = clusters,
samples.per.cluster = samples.per.cluster, sample.fraction = sample.fraction,
mtry = mtry, min.node.size = min.node.size, honesty = honesty,
honesty.fraction = honesty.fraction, honesty.prune.leaves = honesty.prune.leaves,
alpha = alpha, imbalance.penalty = imbalance.penalty,
ci.group.size = ci.group.size, compute.oob.predictions = compute.oob.predictions,
num.threads = num.threads, seed = seed, num_features = num.features,
bandwidth = bandwidth, node_scaling = ifelse(node.scaling,
1, 0))
clusters <- validate_clusters(clusters, X.mat)
samples.per.cluster <- validate_equalize_cluster_weights(equalize.cluster.weights,
clusters, sample.weights)
drf
source("~/GitHub/DRFvarimporance/drf.R")
drf(X=X,Y=Y)
source("~/GitHub/DRFvarimporance/drf.R")
drf(X=X,Y=Y)
source("~/GitHub/DRFvarimporance/drf.R")
drf(X=X,Y=Y)
source("~/GitHub/DRFvarimporance/drf.R")
drf(X=X,Y=Y)
source("~/GitHub/DRFvarimporance/drf.R")
drf(X=X,Y=Y)
source("~/GitHub/DRFvarimporance/drf.R")
drf(X=X,Y=Y)
source("~/GitHub/DRFvarimporance/drf.R")
drf(X=X,Y=Y)
tmp<-drf(X=X,Y=Y)
tmp
tmp
X[1,,drop=F]
predict(tmp,X[1,,drop=F])
predict(tmp,X[1,,drop=F])$weights[1,]
library(grf)
?grf
simulationlist<-simulate_pattern_mixture(m=n, d=3)
X.NA<-simulationlist$X.NA
source("C:/Users/jeffr/OneDrive/Today/MaximinwithMissingvalues/Maximinown.R")
simulationlist<-simulate_pattern_mixture(m=n, d=3)
X.NA<-simulationlist$X.NA
tmp<-drf(X=X.NA,Y=Y)
predict(tmp,X[1,,drop=F])$weights[1,]
validate_X
drf:::validate_X(X.mat)
drf:::validate_X
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
library(ks)
source("drfnew_v2.R")
source("applications")
set.seed(10)
n<-200
ntest<-round(n*0.1)
## Step 1: Get the dataset
tmp<-genData(dataset = "synthetic1", n = n, p = 10, meanShift = 1, sdShift = 1)
source("genData")
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
library(ks)
library(dplyr)
library(kableExtra)
library(copula)
source("drfnew_v2.R")
#source("applications")
source("genData.R")
source("evaluation.R")
set.seed(10)
### Continue with more fancy examples!!!
n<-500
ntest<-round(n*0.1)
num.trees<-3000
##### Continue with the collection of the DRF datasets ######
## Discuss with Julie: Could be super interesting to use a medical dataset with
## patients, whereby X is the patients characteristics and Y is a stream of measurements
##(though would probably need Gaussian processes in a first step)
## Step 2: Do Analysis
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="distshift", L=10, n=n, p=10, num.trees = num.trees)
source("C:/Users/Jeff/OneDrive/Dokumente/Studium/PhD/Projects with Marc/Github/DRFvarimporance/genData.R")
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
library(ks)
library(dplyr)
library(kableExtra)
library(copula)
source("drfnew_v2.R")
#source("applications")
source("genData.R")
source("evaluation.R")
set.seed(10)
### Continue with more fancy examples!!!
n<-500
ntest<-round(n*0.1)
num.trees<-3000
##### Continue with the collection of the DRF datasets ######
## Discuss with Julie: Could be super interesting to use a medical dataset with
## patients, whereby X is the patients characteristics and Y is a stream of measurements
##(though would probably need Gaussian processes in a first step)
## Step 2: Do Analysis
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="GP", L=10, n=n, p=10, num.trees = num.trees)
