d <- 1
# There is a simple and a fancy example
example <- "fancy"
# Sample splitting or not?
sample.splitting <- T
ntest <- n/2
# Define coefficients for the linear combination
# two-dimensional
if (example == "simple") {
if (d == 2) {
B <-
matrix(c(2,-1, 0.5, 0, 0, 0), ncol = 3) # 2 x 3 matrix with chosen coefficients
mu <- c(0, 0) # mean for the random noise
Sigma <-
matrix(c(1, 0.5, 0.5, 1), nrow = 2) # covariance matrix for the random noise
epsilon <-
rmvnorm(n, mu, Sigma) # generate random noise following a bivariate normal distribution
# Create the 2-dimensional response variable Y
Y <-
as.matrix(X %*% t(B) + epsilon) # compute Y as the linear combination of X and B plus random noise
} else if (d == 1) {
# one-dimensional
B <- matrix(c(6, 3, 0), ncol = 3)
epsilon <- rnorm(n)
## Create the 2-dimensional response variable Y
Y <-
as.matrix(X %*% t(B) + epsilon) # compute Y as the linear combination of X and B plus random noise
}
} else if (example == "fancy") {
# more fancy example
sigX = X[, 2]
Y <- matrix(rnorm(n, mean = 0, sd = sqrt(sigX)), nrow = n)
}
if (sample.splitting == T) {
# Sample Splitting
Xtest <- X[(round(n - ntest) + 1):n, , drop = F]
Ytest <- Y[(round(n - ntest) + 1):n, , drop = F]
#
X <- X[1:round(n - ntest), , drop = F]
Y <- Y[1:round(n - ntest), , drop = F]
} else{
# No sample splitting
Xtest <- X
Ytest <- Y
}
B <- 1
num.trees <- 1000
alpha <- 0.05
bandwidth_Y <- drf:::medianHeuristic(Ytest)
k_Y <- rbfdot(sigma = bandwidth_Y)
K <- kernelMatrix(k_Y, Y, y = Y)
DRF <-
drfCI(
X = X,
Y = Y,
B = B,
num.trees = num.trees
)
# Prediction with all X
DRFpred = predictdrf(DRF, x = Xtest)
wall <- DRFpred$weights
##
I0 <- sapply(1:ncol(Xtest), function(j) {
# iterate over class 1
## With CI
DRFj <-
drfCI(
X = X[, -j],
Y = Y,
B = B,
num.trees = num.trees
)
DRFpredj = predictdrf(DRFj, x = Xtest[, -j])
wj <- DRFpredj$weights
val <- mean(diag((wj - wall) %*% K %*% t(wj - wall)))
if (B > 1) {
# Get null distribution if B > 1
nulldist <- sapply(1:B, function(b) {
# iterate over class 1
wbj <- DRFpredj$weightsb[[b]]
wb <- DRFpred$weightsb[[b]]
mean(diag((wb - wall - (wbj - wj)) %*% K %*% t(wb - wall - (wbj - wj))))
})
##
right_quantile <- quantile(nulldist, 1 - alpha)
max(val - unname(right_quantile), 0)
} else{
val
}
})
### Calculate average weight
wbar <- colMeans(wall)
wall_wbar<-sweep(wall, 2, wbar, "-")
#( I<-I0/as.numeric( mean(diag(  as.matrix(wall) %*% K %*% t( as.matrix(wall)) )) - colMeans(wall)%*%K%*%colMeans(wall) ) )
(I <-
I0 / as.numeric(mean(diag(
wall_wbar %*% K %*% t(wall_wbar)
))))
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
source("drfnew_v2.R")
set.seed(10)
# Generate random data for independent variables X
n <- 1000 # number of observations
X <-
matrix(runif(n * 3), ncol = 3) # 100 x 3 matrix with random values from a standard normal distribution
d <- 1
# There is a simple and a fancy example
example <- "fancy"
# Sample splitting or not?
sample.splitting <- F
ntest <- n/2
# Define coefficients for the linear combination
# two-dimensional
if (example == "simple") {
if (d == 2) {
B <-
matrix(c(2,-1, 0.5, 0, 0, 0), ncol = 3) # 2 x 3 matrix with chosen coefficients
mu <- c(0, 0) # mean for the random noise
Sigma <-
matrix(c(1, 0.5, 0.5, 1), nrow = 2) # covariance matrix for the random noise
epsilon <-
rmvnorm(n, mu, Sigma) # generate random noise following a bivariate normal distribution
# Create the 2-dimensional response variable Y
Y <-
as.matrix(X %*% t(B) + epsilon) # compute Y as the linear combination of X and B plus random noise
} else if (d == 1) {
# one-dimensional
B <- matrix(c(6, 3, 0), ncol = 3)
epsilon <- rnorm(n)
## Create the 2-dimensional response variable Y
Y <-
as.matrix(X %*% t(B) + epsilon) # compute Y as the linear combination of X and B plus random noise
}
} else if (example == "fancy") {
# more fancy example
sigX = X[, 2]
Y <- matrix(rnorm(n, mean = 0, sd = sqrt(sigX)), nrow = n)
}
if (sample.splitting == T) {
# Sample Splitting
Xtest <- X[(round(n - ntest) + 1):n, , drop = F]
Ytest <- Y[(round(n - ntest) + 1):n, , drop = F]
#
X <- X[1:round(n - ntest), , drop = F]
Y <- Y[1:round(n - ntest), , drop = F]
} else{
# No sample splitting
Xtest <- X
Ytest <- Y
}
B <- 1
num.trees <- 1000
alpha <- 0.05
bandwidth_Y <- drf:::medianHeuristic(Ytest)
k_Y <- rbfdot(sigma = bandwidth_Y)
K <- kernelMatrix(k_Y, Y, y = Y)
DRF <-
drfCI(
X = X,
Y = Y,
B = B,
num.trees = num.trees
)
# Prediction with all X
DRFpred = predictdrf(DRF, x = Xtest)
wall <- DRFpred$weights
##
I0 <- sapply(1:ncol(Xtest), function(j) {
# iterate over class 1
## With CI
DRFj <-
drfCI(
X = X[, -j],
Y = Y,
B = B,
num.trees = num.trees
)
DRFpredj = predictdrf(DRFj, x = Xtest[, -j])
wj <- DRFpredj$weights
val <- mean(diag((wj - wall) %*% K %*% t(wj - wall)))
if (B > 1) {
# Get null distribution if B > 1
nulldist <- sapply(1:B, function(b) {
# iterate over class 1
wbj <- DRFpredj$weightsb[[b]]
wb <- DRFpred$weightsb[[b]]
mean(diag((wb - wall - (wbj - wj)) %*% K %*% t(wb - wall - (wbj - wj))))
})
##
right_quantile <- quantile(nulldist, 1 - alpha)
max(val - unname(right_quantile), 0)
} else{
val
}
})
### Calculate average weight
wbar <- colMeans(wall)
wall_wbar<-sweep(wall, 2, wbar, "-")
#( I<-I0/as.numeric( mean(diag(  as.matrix(wall) %*% K %*% t( as.matrix(wall)) )) - colMeans(wall)%*%K%*%colMeans(wall) ) )
(I <-
I0 / as.numeric(mean(diag(
wall_wbar %*% K %*% t(wall_wbar)
))))
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
source("drfnew_v2.R")
set.seed(10)
# Generate random data for independent variables X
n <- 1000 # number of observations
X <-
matrix(runif(n * 3), ncol = 3) # 100 x 3 matrix with random values from a standard normal distribution
d <- 1
# There is a simple and a fancy example
example <- "fancy"
# Sample splitting or not?
sample.splitting <- F
ntest <- n/2
# Define coefficients for the linear combination
# two-dimensional
if (example == "simple") {
if (d == 2) {
B <-
matrix(c(2,-1, 0.5, 0, 0, 0), ncol = 3) # 2 x 3 matrix with chosen coefficients
mu <- c(0, 0) # mean for the random noise
Sigma <-
matrix(c(1, 0.5, 0.5, 1), nrow = 2) # covariance matrix for the random noise
epsilon <-
rmvnorm(n, mu, Sigma) # generate random noise following a bivariate normal distribution
# Create the 2-dimensional response variable Y
Y <-
as.matrix(X %*% t(B) + epsilon) # compute Y as the linear combination of X and B plus random noise
} else if (d == 1) {
# one-dimensional
B <- matrix(c(6, 3, 0), ncol = 3)
epsilon <- rnorm(n)
## Create the 2-dimensional response variable Y
Y <-
as.matrix(X %*% t(B) + epsilon) # compute Y as the linear combination of X and B plus random noise
}
} else if (example == "fancy") {
# more fancy example
sigX = X[, 2]
Y <- matrix(rnorm(n, mean = 0, sd = sqrt(sigX)), nrow = n)
}
if (sample.splitting == T) {
# Sample Splitting
Xtest <- X[(round(n - ntest) + 1):n, , drop = F]
Ytest <- Y[(round(n - ntest) + 1):n, , drop = F]
#
X <- X[1:round(n - ntest), , drop = F]
Y <- Y[1:round(n - ntest), , drop = F]
} else{
# No sample splitting
Xtest <- X
Ytest <- Y
}
B <- 1
num.trees <- 1000
alpha <- 0.05
bandwidth_Y <- drf:::medianHeuristic(Ytest)
k_Y <- rbfdot(sigma = bandwidth_Y)
K <- kernelMatrix(k_Y, Y, y = Y)
DRF <-
drfCI(
X = X,
Y = Y,
B = B,
num.trees = num.trees
)
# Prediction with all X
DRFpred = predictdrf(DRF, x = Xtest)
wall <- DRFpred$weights
##
I0 <- sapply(1:ncol(Xtest), function(j) {
# iterate over class 1
## With CI
DRFj <-
drfCI(
X = X[, -j],
Y = Y,
B = B,
num.trees = num.trees
)
DRFpredj = predictdrf(DRFj, x = Xtest[, -j])
wj <- DRFpredj$weights
val <- sum(diag( (wj - wall) %*% K %*% t(wj - wall) ))
if (B > 1) {
# Get null distribution if B > 1
nulldist <- sapply(1:B, function(b) {
# iterate over class 1
wbj <- DRFpredj$weightsb[[b]]
wb <- DRFpred$weightsb[[b]]
sum(diag((wb - wall - (wbj - wj)) %*% K %*% t(wb - wall - (wbj - wj))))
})
##
right_quantile <- quantile(nulldist, 1 - alpha)
max(val - unname(right_quantile), 0)
} else{
val
}
})
### Calculate average weight
wbar <- colMeans(wall)
wall_wbar<-sweep(wall, 2, wbar, "-")
#( I<-I0/as.numeric( mean(diag(  as.matrix(wall) %*% K %*% t( as.matrix(wall)) )) - colMeans(wall)%*%K%*%colMeans(wall) ) )
(I <-
I0 / as.numeric(sum(diag(
wall_wbar %*% K %*% t(wall_wbar)
))))
### Calculate average weight
wbar <- colMeans(wall)
wbar
1/1000
wall[,1]
DRF2<-drf(X,Y, num.trees=num.trees)
weightsall<-predict(DRF2)$weights
weightsall
colMeans(weightsall)
1/1000
Xtest
DRF <-
drfCI(
X = X,
Y = Y,
B = B,
num.trees = num.trees
)
# Prediction with all X
DRFpred = predictdrf(DRF, x = Xtest)
wall <- DRFpred$weights
##
wall[1,]
weightsall[1,]
wall[1,]-weightsall[1,]
max(wall[1,]-weightsall[1,])
B
n
B<.10
B<-10
DRF <-
drfCI(
X = X,
Y = Y,
B = B,
num.trees = num.trees
)
# Prediction with all X
DRFpred = predictdrf(DRF, x = Xtest)
wall <- DRFpred$weights
##
### Calculate average weight
wbar <- colMeans(wall)
wall_wbar<-sweep(wall, 2, wbar, "-")
#( I<-I0/as.numeric( mean(diag(  as.matrix(wall) %*% K %*% t( as.matrix(wall)) )) - colMeans(wall)%*%K%*%colMeans(wall) ) )
(I <-
I0 / as.numeric(sum(diag(
wall_wbar %*% K %*% t(wall_wbar)
))))
wbar
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
source("drfnew_v2.R")
set.seed(10)
# Generate random data for independent variables X
n <- 1000 # number of observations
X <-
matrix(runif(n * 3), ncol = 3) # 100 x 3 matrix with random values from a standard normal distribution
d <- 1
# There is a simple and a fancy example
example <- "fancy"
# Sample splitting or not?
sample.splitting <- F
ntest <- n/2
# Define coefficients for the linear combination
# two-dimensional
if (example == "simple") {
if (d == 2) {
B <-
matrix(c(2,-1, 0.5, 0, 0, 0), ncol = 3) # 2 x 3 matrix with chosen coefficients
mu <- c(0, 0) # mean for the random noise
Sigma <-
matrix(c(1, 0.5, 0.5, 1), nrow = 2) # covariance matrix for the random noise
epsilon <-
rmvnorm(n, mu, Sigma) # generate random noise following a bivariate normal distribution
# Create the 2-dimensional response variable Y
Y <-
as.matrix(X %*% t(B) + epsilon) # compute Y as the linear combination of X and B plus random noise
} else if (d == 1) {
# one-dimensional
b <- matrix(c(6, 3, 0), ncol = 3)
epsilon <- rnorm(n)
## Create the 2-dimensional response variable Y
Y <-
as.matrix(X %*% t(b) + epsilon) # compute Y as the linear combination of X and B plus random noise
}
} else if (example == "fancy") {
# more fancy example
sigX = X[, 2]
Y <- matrix(rnorm(n, mean = 0, sd = sqrt(sigX)), nrow = n)
}
if (sample.splitting == T) {
# Sample Splitting
Xtest <- X[(round(n - ntest) + 1):n, , drop = F]
Ytest <- Y[(round(n - ntest) + 1):n, , drop = F]
#
X <- X[1:round(n - ntest), , drop = F]
Y <- Y[1:round(n - ntest), , drop = F]
} else{
# No sample splitting
Xtest <- X
Ytest <- Y
}
B <- 1
num.trees <- 1000
alpha <- 0.05
bandwidth_Y <- drf:::medianHeuristic(Ytest)
k_Y <- rbfdot(sigma = bandwidth_Y)
K <- kernelMatrix(k_Y, Y, y = Y)
if (B > 1){
DRF <-
drfCI(
X = X,
Y = Y,
B = B,
num.trees = num.trees
)
# Prediction with all X
DRFpred = predictdrf(DRF, x = Xtest)
wall <- DRFpred$weights
##
I0 <- sapply(1:ncol(Xtest), function(j) {
# iterate over class 1
## With CI
DRFj <-
drfCI(
X = X[, -j],
Y = Y,
B = B,
num.trees = num.trees
)
DRFpredj = predictdrf(DRFj, x = Xtest[, -j])
wj <- DRFpredj$weights
val <- sum(diag( (wj - wall) %*% K %*% t(wj - wall) ))
# Get null distribution if B > 1
nulldist <- sapply(1:B, function(b) {
# iterate over class 1
wbj <- DRFpredj$weightsb[[b]]
wb <- DRFpred$weightsb[[b]]
sum(diag((wb - wall - (wbj - wj)) %*% K %*% t(wb - wall - (wbj - wj))))
})
##
right_quantile <- quantile(nulldist, 1 - alpha)
max(val - unname(right_quantile), 0)
})
}else{
DRF2<-drf(X,Y, num.trees=num.trees)
wall<-predict(DRF2,x = Xtest)$weights
}
### Calculate average weight
wbar <- colMeans(wall)
wall_wbar<-sweep(wall, 2, wbar, "-")
#( I<-I0/as.numeric( mean(diag(  as.matrix(wall) %*% K %*% t( as.matrix(wall)) )) - colMeans(wall)%*%K%*%colMeans(wall) ) )
(I <-
I0 / as.numeric(sum(diag(
wall_wbar %*% K %*% t(wall_wbar)
))))
wall
I0 <- sapply(1:ncol(Xtest), function(j) {
# iterate over class 1
## With CI
DRFj <-
drf(
X = X[, -j],
Y = Y,
num.trees = num.trees
)
DRFpredj = predict(DRFj, x = Xtest[, -j])
wj <- DRFpredj$weights
val <- sum(diag( (wj - wall) %*% K %*% t(wj - wall) ))
})
I0
### Calculate average weight
wbar <- colMeans(wall)
wall_wbar<-sweep(wall, 2, wbar, "-")
wbar
#( I<-I0/as.numeric( mean(diag(  as.matrix(wall) %*% K %*% t( as.matrix(wall)) )) - colMeans(wall)%*%K%*%colMeans(wall) ) )
(I <-
I0 / as.numeric(sum(diag(
wall_wbar %*% K %*% t(wall_wbar)
))))
n
# alternative:
wbar<- rep(1/n)
# alternative:
wbar<- rep(1/n,n)
wbar
wall_wbar<-sweep(wall, 2, wbar, "-")
#( I<-I0/as.numeric( mean(diag(  as.matrix(wall) %*% K %*% t( as.matrix(wall)) )) - colMeans(wall)%*%K%*%colMeans(wall) ) )
(I <-
I0 / as.numeric(sum(diag(
wall_wbar %*% K %*% t(wall_wbar)
))))
