}
Lossall
LossX1
DRFall<-drf(X=Xtrain, Y=Ytrain, num.trees=num.trees)
quantpredictall<-predict(DRF, newdata=Xtest, functional="quantile",quantiles=c(0.025, 0.5, 0.975))
# only with X_1
DRFX1<-drf(X=Xtrain[,-2,drop=F], Y=Ytrain, num.trees=num.trees)
quantpredictX1<-predict(DRFX1, newdata=Xtest, functional="quantile",quantiles=c(0.025, 0.5, 0.975))
### Compare to actual quantiles:
i<-0
Lossall<-matrix(NaN, nrow=3)
LossX1<-matrix(NaN, nrow=3)
for (tau in c(0.025, 0.5, 0.975)){
i<-i+1
Losslist<-lapply(1:nrow(Xtest),  function(j)  {
truth=qnorm(tau, mean= 0.8*(Xtest[j,1] > 0), sd = sqrt( 1 + (Xtest[j,2] > 0) ) )
eall<-quantpredictall$quantile[,,i][j]
eX1 <-quantpredictX1$quantile[,,i][j]
return( list(Lossall= (truth - eall)*tau*(truth >= eall)+
(eall- truth )*(1-tau)*(truth < eall),
LossX1 = (truth - eX1)*tau*(truth >= eX1)+
(eX1- truth )*(1-tau)*(truth < eX1)))
}  )
Lossall[i]<-mean(sapply(1:nrow(Xtest), function(j)  Losslist[[j]]$Lossall ))
LossX1[i]<-mean(sapply(1:nrow(Xtest), function(j)  Losslist[[j]]$LossX1 ))
}
Lossall
LossX1
(LossX1-Lossall)/Lossall
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
library(ks)
library(dplyr)
library(kableExtra)
library(copula)
source("drfnew_v2.R")
#source("applications")
source("genData.R")
source("evaluation.R")
set.seed(10)
### Continue with more fancy examples!!!
n<-500
ntest<-round(n*0.1)
num.trees<-2000
##### Continue with the collection of the DRF datasets ######
## Discuss with Julie: Could be super interesting to use a medical dataset with
## patients, whereby X is the patients characteristics and Y is a stream of measurements
##(though would probably need Gaussian processes in a first step)
## Step 2: Do Analysis
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="motivatingexample", L=10, n=n, B=1, p=2, num.trees = num.trees)
## Step 3: Check quantile error when using only X_1 instead of X_1 and X_2.
tmp<-genData(dataset = "motivatingexample", n = 2*n, p = 2)
Xtrain<-tmp$X[1:n,]
Xtest<-tmp$X[(n+1):(2*n),]
Ytrain<-as.matrix(tmp$y[1:n])
Ytest <-as.matrix(tmp$y[(n+1):(2*n)])
colnames(Xtrain) <- paste0("X",1:ncol(X))
# target quantiles: 0.025, 0.5, 0.975
DRFall<-drf(X=Xtrain, Y=Ytrain, num.trees=num.trees)
quantpredictall<-predict(DRF, newdata=Xtest, functional="quantile",quantiles=c(0.025, 0.5, 0.975))
# only with X_1
DRFX1<-drf(X=Xtrain[,-2,drop=F], Y=Ytrain, num.trees=num.trees)
quantpredictX1<-predict(DRFX1, newdata=Xtest, functional="quantile",quantiles=c(0.025, 0.5, 0.975))
### Compare to actual quantiles:
i<-0
Lossall<-matrix(NaN, nrow=3)
LossX1<-matrix(NaN, nrow=3)
for (tau in c(0.025, 0.5, 0.975)){
i<-i+1
Losslist<-lapply(1:nrow(Xtest),  function(j)  {
truth=qnorm(tau, mean= 0.8*(Xtest[j,1] > 0), sd = sqrt( 1 + (Xtest[j,2] > 0) ) )
eall<-quantpredictall$quantile[,,i][j]
eX1 <-quantpredictX1$quantile[,,i][j]
return( list(Lossall= (truth - eall)*tau*(truth >= eall)+
(eall- truth )*(1-tau)*(truth < eall),
LossX1 = (truth - eX1)*tau*(truth >= eX1)+
(eX1- truth )*(1-tau)*(truth < eX1)))
}  )
Lossall[i]<-mean(sapply(1:nrow(Xtest), function(j)  Losslist[[j]]$Lossall ))
LossX1[i]<-mean(sapply(1:nrow(Xtest), function(j)  Losslist[[j]]$LossX1 ))
}
Lossall
LossX1
(LossX1-Lossall)/Lossall
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
library(ks)
library(dplyr)
library(kableExtra)
library(copula)
source("drfnew_v2.R")
#source("applications")
source("genData.R")
source("evaluation.R")
set.seed(10)
### Continue with more fancy examples!!!
n<-500
ntest<-round(n*0.1)
num.trees<-2000
##### Continue with the collection of the DRF datasets ######
## Discuss with Julie: Could be super interesting to use a medical dataset with
## patients, whereby X is the patients characteristics and Y is a stream of measurements
##(though would probably need Gaussian processes in a first step)
## Step 2: Do Analysis
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="motivatingexample", L=10, n=n, B=1, p=2, num.trees = num.trees)
## Step 3: Check quantile error when using only X_1 instead of X_1 and X_2.
tmp<-genData(dataset = "motivatingexample", n = 2*n, p = 2)
Xtrain<-tmp$X[1:n,]
Xtest<-tmp$X[(n+1):(2*n),]
Ytrain<-as.matrix(tmp$y[1:n])
Ytest <-as.matrix(tmp$y[(n+1):(2*n)])
colnames(Xtrain) <- paste0("X",1:ncol(X))
# target quantiles: 0.025, 0.5, 0.975
DRFall<-drf(X=Xtrain, Y=Ytrain, num.trees=num.trees)
quantpredictall<-predict(DRFall, newdata=Xtest, functional="quantile",quantiles=c(0.025, 0.5, 0.975))
# only with X_1
DRFX1<-drf(X=Xtrain[,-2,drop=F], Y=Ytrain, num.trees=num.trees)
quantpredictX1<-predict(DRFX1, newdata=Xtest, functional="quantile",quantiles=c(0.025, 0.5, 0.975))
### Compare to actual quantiles:
i<-0
Lossall<-matrix(NaN, nrow=3)
LossX1<-matrix(NaN, nrow=3)
for (tau in c(0.025, 0.5, 0.975)){
i<-i+1
Losslist<-lapply(1:nrow(Xtest),  function(j)  {
truth=qnorm(tau, mean= 0.8*(Xtest[j,1] > 0), sd = sqrt( 1 + (Xtest[j,2] > 0) ) )
eall<-quantpredictall$quantile[,,i][j]
eX1 <-quantpredictX1$quantile[,,i][j]
return( list(Lossall= (truth - eall)*tau*(truth >= eall)+
(eall- truth )*(1-tau)*(truth < eall),
LossX1 = (truth - eX1)*tau*(truth >= eX1)+
(eX1- truth )*(1-tau)*(truth < eX1)))
}  )
Lossall[i]<-mean(sapply(1:nrow(Xtest), function(j)  Losslist[[j]]$Lossall ))
LossX1[i]<-mean(sapply(1:nrow(Xtest), function(j)  Losslist[[j]]$LossX1 ))
}
Lossall
LossX1
(LossX1-Lossall)/Lossall
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
library(ks)
library(dplyr)
library(kableExtra)
library(copula)
source("drfnew_v2.R")
#source("applications")
source("genData.R")
source("evaluation.R")
set.seed(10)
### Continue with more fancy examples!!!
n<-500
ntest<-round(n*0.1)
num.trees<-2000
##### Continue with the collection of the DRF datasets ######
## Discuss with Julie: Could be super interesting to use a medical dataset with
## patients, whereby X is the patients characteristics and Y is a stream of measurements
##(though would probably need Gaussian processes in a first step)
## Step 2: Do Analysis
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="motivatingexample", L=10, n=n, B=1, p=2, num.trees = num.trees)
## Step 3: Check quantile error when using only X_1 instead of X_1 and X_2.
tmp<-genData(dataset = "motivatingexample", n = 2*n, p = 2)
Xtrain<-tmp$X[1:n,]
Xtest<-tmp$X[(n+1):(2*n),]
Ytrain<-as.matrix(tmp$y[1:n])
Ytest <-as.matrix(tmp$y[(n+1):(2*n)])
colnames(Xtrain) <- paste0("X",1:ncol(X))
# target quantiles: 0.025, 0.5, 0.975
DRFall<-drf(X=Xtrain, Y=Ytrain, num.trees=num.trees)
quantpredictall<-predict(DRFall, newdata=Xtest, functional="quantile",quantiles=c(0.025, 0.5, 0.975))
# only with X_1
DRFX1<-drf(X=Xtrain[,1,drop=F], Y=Ytrain, num.trees=num.trees)
quantpredictX1<-predict(DRFX1, newdata=Xtest[,1, drop=F], functional="quantile",quantiles=c(0.025, 0.5, 0.975))
### Compare to actual quantiles:
i<-0
Lossall<-matrix(NaN, nrow=3)
LossX1<-matrix(NaN, nrow=3)
for (tau in c(0.025, 0.5, 0.975)){
i<-i+1
Losslist<-lapply(1:nrow(Xtest),  function(j)  {
truth=qnorm(tau, mean= 0.8*(Xtest[j,1] > 0), sd = sqrt( 1 + (Xtest[j,2] > 0) ) )
eall<-quantpredictall$quantile[,,i][j]
eX1 <-quantpredictX1$quantile[,,i][j]
return( list(Lossall= (truth - eall)*tau*(truth >= eall)+
(eall- truth )*(1-tau)*(truth < eall),
LossX1 = (truth - eX1)*tau*(truth >= eX1)+
(eX1- truth )*(1-tau)*(truth < eX1)))
}  )
Lossall[i]<-mean(sapply(1:nrow(Xtest), function(j)  Losslist[[j]]$Lossall ))
LossX1[i]<-mean(sapply(1:nrow(Xtest), function(j)  Losslist[[j]]$LossX1 ))
}
Lossall
LossX1
(LossX1-Lossall)/Lossall
tmp<-genData(dataset = "motivatingexample", n = 2*n, p = 2)
Xtrain<-tmp$X[1:n,]
Xtest<-tmp$X[(n+1):(2*n),]
Ytrain<-as.matrix(tmp$y[1:n])
Ytest <-as.matrix(tmp$y[(n+1):(2*n)])
colnames(Xtrain) <- paste0("X",1:ncol(X))
# target quantiles: 0.025, 0.5, 0.975
DRFall<-drf(X=Xtrain, Y=Ytrain, num.trees=num.trees)
quantpredictall<-predict(DRFall, newdata=Xtest, functional="quantile",quantiles=c(0.025, 0.5, 0.975))
# only with X_1
DRFX1<-drf(X=Xtrain[,1,drop=F], Y=Ytrain, num.trees=num.trees)
quantpredictX1<-predict(DRFX1, newdata=Xtest[,1, drop=F], functional="quantile",quantiles=c(0.025, 0.5, 0.975))
### Compare to actual quantiles:
i<-0
Lossall<-matrix(NaN, nrow=3)
LossX1<-matrix(NaN, nrow=3)
for (tau in c(0.025, 0.5, 0.975)){
i<-i+1
Losslist<-lapply(1:nrow(Xtest),  function(j)  {
truth=qnorm(tau, mean= 0.8*(Xtest[j,1] > 0), sd = sqrt( 1 + (Xtest[j,2] > 0) ) )
eall<-quantpredictall$quantile[,,i][j]
eX1 <-quantpredictX1$quantile[,,i][j]
return( list(Lossall= (truth - eall)*tau*(truth >= eall)+
(eall- truth )*(1-tau)*(truth < eall),
LossX1 = (truth - eX1)*tau*(truth >= eX1)+
(eX1- truth )*(1-tau)*(truth < eX1)))
}  )
Lossall[i]<-mean(sapply(1:nrow(Xtest), function(j)  Losslist[[j]]$Lossall ))
LossX1[i]<-mean(sapply(1:nrow(Xtest), function(j)  Losslist[[j]]$LossX1 ))
}
Lossall
LossX1
(LossX1-Lossall)/Lossall
tmp<-genData(dataset = "motivatingexample", n = 2*n, p = 2)
Xtrain<-tmp$X[1:n,]
Xtest<-tmp$X[(n+1):(2*n),]
Ytrain<-as.matrix(tmp$y[1:n])
Ytest <-as.matrix(tmp$y[(n+1):(2*n)])
colnames(Xtrain) <- paste0("X",1:ncol(X))
DRFall<-drf(X=Xtrain, Y=Ytrain, num.trees=num.trees)
colnames(Xtrain) <- paste0("X",1:ncol(Xtrain))
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
library(ks)
library(dplyr)
library(kableExtra)
library(copula)
source("drfnew_v2.R")
#source("applications")
source("genData.R")
source("evaluation.R")
set.seed(10)
### Continue with more fancy examples!!!
n<-500
ntest<-round(n*0.1)
num.trees<-2000
##### Continue with the collection of the DRF datasets ######
## Discuss with Julie: Could be super interesting to use a medical dataset with
## patients, whereby X is the patients characteristics and Y is a stream of measurements
##(though would probably need Gaussian processes in a first step)
## Step 2: Do Analysis
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="motivatingexample", L=10, n=n, B=1, p=2, num.trees = num.trees)
## Step 3: Check quantile error when using only X_1 instead of X_1 and X_2.
tmp<-genData(dataset = "motivatingexample", n = 2*n, p = 2)
Xtrain<-tmp$X[1:n,]
Xtest<-tmp$X[(n+1):(2*n),]
Ytrain<-as.matrix(tmp$y[1:n])
Ytest <-as.matrix(tmp$y[(n+1):(2*n)])
colnames(Xtrain) <- paste0("X",1:ncol(Xtrain))
# target quantiles: 0.025, 0.5, 0.975
DRFall<-drf(X=Xtrain, Y=Ytrain, num.trees=num.trees)
quantpredictall<-predict(DRFall, newdata=Xtest, functional="quantile",quantiles=c(0.025, 0.5, 0.975))
# only with X_1
DRFX1<-drf(X=Xtrain[,1,drop=F], Y=Ytrain, num.trees=num.trees)
quantpredictX1<-predict(DRFX1, newdata=Xtest[,1, drop=F], functional="quantile",quantiles=c(0.025, 0.5, 0.975))
### Compare to actual quantiles:
i<-0
Lossall<-matrix(NaN, nrow=3)
LossX1<-matrix(NaN, nrow=3)
for (tau in c(0.025, 0.5, 0.975)){
i<-i+1
Losslist<-lapply(1:nrow(Xtest),  function(j)  {
truth=qnorm(tau, mean= 0.8*(Xtest[j,1] > 0), sd = sqrt( 1 + (Xtest[j,2] > 0) ) )
eall<-quantpredictall$quantile[,,i][j]
eX1 <-quantpredictX1$quantile[,,i][j]
return( list(Lossall= (truth - eall)*tau*(truth >= eall)+
(eall- truth )*(1-tau)*(truth < eall),
LossX1 = (truth - eX1)*tau*(truth >= eX1)+
(eX1- truth )*(1-tau)*(truth < eX1)))
}  )
Lossall[i]<-mean(sapply(1:nrow(Xtest), function(j)  Losslist[[j]]$Lossall ))
LossX1[i]<-mean(sapply(1:nrow(Xtest), function(j)  Losslist[[j]]$LossX1 ))
}
Lossall
LossX1
(LossX1-Lossall)/Lossall
install.packages("MulvariateRandomForestVarImp")
library("MulvariateRandomForestVarImp")
debugSource("C:/Users/Jeff/OneDrive/Dokumente/Studium/PhD/Projects with Marc/Github/DRFvarimporance/evaluation.R")
library(kernlab)
library(drf)
library(Matrix)
# Load necessary libraries
library(mvtnorm) # for generating multivariate normal random variables
library(ks)
library(dplyr)
library(kableExtra)
library(copula)
library("MulvariateRandomForestVarImp")
source("drfnew_v2.R")
#source("applications")
source("genData.R")
source("evaluation.R")
set.seed(10)
### Continue with more fancy examples!!!
n<-500
ntest<-round(n*0.1)
num.trees<-2000
##### Continue with the collection of the DRF datasets ######
## Discuss with Julie: Could be super interesting to use a medical dataset with
## patients, whereby X is the patients characteristics and Y is a stream of measurements
##(though would probably need Gaussian processes in a first step)
## Step 2: Do Analysis
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="distshift", L=10, n=n, B=50, p=3, num.trees = num.trees)
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="distshift", L=10, n=n, B=1, p=3, num.trees = num.trees)
debug(evalsynthetic)
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="distshift", L=10, n=n, B=1, p=3, num.trees = num.trees)
debugSource("C:/Users/Jeff/OneDrive/Dokumente/Studium/PhD/Projects with Marc/Github/DRFvarimporance/evaluation.R")
debug(evalsynthetic)
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="distshift", L=10, n=n, B=1, p=3, num.trees = num.trees)
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="distshift", L=10, n=n, B=1, p=3, num.trees = num.trees)
X
Y
?MeanOutcomeDifference
MeanOutcomeDifference(X, Y, num_trees=num.trees)
Y = matrix(runif(50*2), 50, 2)
MeanOutcomeDifference(X, Y, num_trees=num.trees)
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="copulasynthetic", L=10, n=n, B=1, p=3, num.trees = num.trees)
MeanOutcomeDifference(X, Y, num_trees=num.trees)
MeanOutcomeDifference(X, Y, num_trees=100)
dim(X)
dim(Y)
MeanSplitImprovement(X, Y, num_trees=num.trees)
MeanSplitImprovement(X, Y, num_trees=100)
debugSource("C:/Users/Jeff/OneDrive/Dokumente/Studium/PhD/Projects with Marc/Github/DRFvarimporance/evaluation.R")
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="copulasynthetic", L=10, n=n, B=1, p=10, num.trees = num.trees)
source("C:/Users/Jeff/OneDrive/Dokumente/Studium/PhD/Projects with Marc/Github/DRFvarimporance/DRFvariableimportance.R")
debugSource("C:/Users/Jeff/OneDrive/Dokumente/Studium/PhD/Projects with Marc/Github/DRFvarimporance/evaluation.R")
### 2.a) if the dataset is synthetic, we can check the correct variable ordering
evalsynthetic(dataset="copulasynthetic", L=10, n=n, B=1, p=10, num.trees = num.trees, MRF=T)
ressynth$VI
nrow(X)
num.trees
num.features
library(MulvariateRandomForestVarImp)
## basic example code
set.seed(49)
X <- matrix(runif(50*5), 50, 5)
Y <- matrix(runif(50*2), 50, 2)
start.time <- Sys.time()
split_improvement_importance <- MeanSplitImprovement(X, Y)
split_improvement_importance
#> [1] 0.8066173 2.8909635 3.4591123 0.6227943 0.5138745
mean_outccome_diff_importance <- MeanOutcomeDifference(X, Y)
mean_outccome_diff_importance
#>           [,1]      [,2]
#> [1,] 0.2458139 0.3182474
#> [2,] 0.2712269 0.2915053
#> [3,] 0.2125802 0.2023291
#> [4,] 0.2819759 0.2519035
#> [5,] 0.1238451 0.1958629
end.time <- Sys.time()
MeanSplitImprovement
X
dim(X)
dim(Y)
n<-100
X <- matrix(runif(n*5), n, 5)
Y <- matrix(runif(n*2), n, 2)
start.time <- Sys.time()
split_improvement_importance <- MeanSplitImprovement(X, Y)
split_improvement_importance
mean_outccome_diff_importance <- MeanOutcomeDifference(X, Y)
mean_outccome_diff_importance
end.time <- Sys.time()
library(MulvariateRandomForestVarImp)
## basic example code
set.seed(49)
n<-100
X <- matrix(runif(n*5), n, 5)
Y <- matrix(runif(n*2), n, 2)
start.time <- Sys.time()
split_improvement_importance <- MeanSplitImprovement(X, Y)
split_improvement_importance
#> [1] 0.8066173 2.8909635 3.4591123 0.6227943 0.5138745
mean_outccome_diff_importance <- MeanOutcomeDifference(X, Y)
mean_outccome_diff_importance
#>           [,1]      [,2]
#> [1,] 0.2458139 0.3182474
#> [2,] 0.2712269 0.2915053
#> [3,] 0.2125802 0.2023291
#> [4,] 0.2819759 0.2519035
#> [5,] 0.1238451 0.1958629
end.time <- Sys.time()
start.time-end.time
library(MulvariateRandomForestVarImp)
## basic example code
set.seed(49)
n<-100
X <- matrix(runif(n*5), n, 5)
Y <- matrix(runif(n*2), n, 2)
start.time <- Sys.time()
split_improvement_importance <- MeanSplitImprovement(X, Y)
split_improvement_importance
#> [1] 0.8066173 2.8909635 3.4591123 0.6227943 0.5138745
mean_outccome_diff_importance <- MeanOutcomeDifference(X, Y)
mean_outccome_diff_importance
#>           [,1]      [,2]
#> [1,] 0.2458139 0.3182474
#> [2,] 0.2712269 0.2915053
#> [3,] 0.2125802 0.2023291
#> [4,] 0.2819759 0.2519035
#> [5,] 0.1238451 0.1958629
end.time <- Sys.time()
end.time-start.time
library(MulvariateRandomForestVarImp)
## basic example code
set.seed(49)
n<-200
X <- matrix(runif(n*5), n, 5)
Y <- matrix(runif(n*2), n, 2)
start.time <- Sys.time()
split_improvement_importance <- MeanSplitImprovement(X, Y)
split_improvement_importance
#> [1] 0.8066173 2.8909635 3.4591123 0.6227943 0.5138745
mean_outccome_diff_importance <- MeanOutcomeDifference(X, Y)
mean_outccome_diff_importance
#>           [,1]      [,2]
#> [1,] 0.2458139 0.3182474
#> [2,] 0.2712269 0.2915053
#> [3,] 0.2125802 0.2023291
#> [4,] 0.2819759 0.2519035
#> [5,] 0.1238451 0.1958629
end.time <- Sys.time()
end.time-start.time
library(MulvariateRandomForestVarImp)
## basic example code
set.seed(49)
n<-200
X <- matrix(runif(n*5), n, 5)
Y <- matrix(runif(n*2), n, 2)
## DRF brute force
start.time <- Sys.time()
ressynth<-drfwithVI(X, Y, B=1, num.trees=2000, num.features=10)
ressynth$VI
end.time <- Sys.time()
end.time-start.time
start.time <- Sys.time()
split_improvement_importance <- MeanSplitImprovement(X, Y)
split_improvement_importance
#> [1] 0.8066173 2.8909635 3.4591123 0.6227943 0.5138745
end.time <- Sys.time()
end.time-start.time
X <- runif(n,-1,1)
Y <- ifelse(x >= 0, rexp(n = n, 1), rnorm(n, 1, 1))
DRF<-drf(X,Y)
n<-1000
X <- runif(n,-1,1)
Y <- ifelse(X >= 0, rexp(n = n, 1), rnorm(n, 1, 1))
DRF<-drf(X,Y)
DRF<-drf(X,Y)
n<-1000
X <- matrix(runif(n,-1,1), nrow=n)
Y <- matrix(ifelse(X >= 0, rexp(n = n, 1), rnorm(n, 1, 1)), nrow=n)
DRF<-drf(X,Y)
predict(DRF, newdata=0)
weights<-predict(DRF, newdata=0)$weights
weights
Y[sample(1:n, replace = T,size = n, prob = weights),]
sample(1:n, replace = T,size = n, prob = weights)
weights
sample(1:n, replace = T,size = n, prob = c(weights))
sample(1:n, replace = T,size = 2, prob = c(weights))
c(weights)
sample(1:n, replace = T,size = n, prob = weights[1,])
n<-1000
X <- matrix(runif(n,-1,1), nrow=n)
Y <- matrix(ifelse(X >= 0, rexp(n = n, 1), rnorm(n, 1, 1)), nrow=n)
DRF<-drf(X,Y)
weights<-predict(DRF, newdata=0.5)$weights
Yx<-Y[sample(1:n, replace = T,size = n/2, prob = weights[1,]),]
hist(Yx)
weights<-predict(DRF, newdata=-0.5)$weights
Yx<-Y[sample(1:n, replace = T,size = n/2, prob = weights[1,]),]
hist(Yx)
